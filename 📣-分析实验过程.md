---
title: 📣 分析实验过程
date: 2023-09-04 17:50:28
tags: backdoors101
categories: 论文实验
---

# 1.联邦学习过程

从main出发——> fl_run（联邦学习总的训练过程）——> run_fl_round（联邦学习的一轮训练）——>test（每轮进行测试--测试正常样本和后门样本）——>save_model（保存模型）

# 2.联邦学习一轮训练

## 1.helper里能干的事情

1. 初始化任务
   - 给params赋值
   - `make_task` 找到运行联邦任务的py文件
   - `make_synthesizer`找到后门攻击对应的py文件，并初始化攻击
2. 保存模型
3. 保存断点

## 2.`run_fl_round`中干的事情

1. 加载全局模型和局部模型
2. 随机选择参与者（可能包括恶意参与者）：fl_task文件中
3. 创建一个新的权重累积字典
4. 接着每个参与者在本地训练全局模型，得到上传更新
5. 然后更新全局模型

# 3.对参与者进行中毒和随机选择参与者

## 1.随机选择10个客户端

**实现**：`fl_task`文件中`sample_users_for_round`

里面的实现思路：

1. 在100个参与者中随机选择10个参与者，是从100个id中选择10个id
2. 新建一个参与者样本列表
3. 对于这10个用户，来查看其是否是恶意的，并对每个用户打标记（恶意和非恶意）
4. 将打了标记的10个参与者加入参与者样本列表

## 2.给参与者打标记

**实现**：`fl_task`文件中`check_user_compromised`

里面的实现思路：

1. 先让参与者为正常参与者
2. 根据单轮攻击还是多轮攻击中的攻击者列表来给当前参与者打标记
3. 返回标记

# 4.参与者本地的训练过程

**实现**：`training`文件里

实现思路如下：

1. 将全局模型复制到本地模型中，实现：`fl_task`文件中`copy_params`
2. 选择优化器SGD-随机梯度下降算法，用于优化损失函数，实现：`task`文件中的`make_optimizer`
3. 开始本地轮次的训练：
   1. 如果是恶意参与者，就怎么训练
   2. 如果是正常参与者，就怎么训练
4. 计算本地更新，
   1. 正常参与者，实现：`fl_task`文件中`get_fl_update`。
   2. 恶意参与者，还会乘上一个数，实现：`attack`文件中的`fl_scale_update`
5. 积累权重然后记录到权重累积字典，实现：`fl_task`文件中`accumulate_weights`
6. 更新全局模型，实现：`fl_task`文件中`update_global_model`

# 5.本地模型训练的过程

**实现**：training文件中

实现的思路如下：

1. 确定损失函数：交叉熵，实现：`task`文件中的`make_criterion`
2. 开启训练模式
3. 开始本地轮次的批次训练：
   1. 获得当前的batch，把数据加载到设备上
   2. 梯度为0
   3. 计算损失，实现：`attack.py`中的`compute_blind_loss`
   4. 将损失loss 向输入侧进行反向传播
   5. SGD优化器对值进行更新
   6. 打印训练时期的精度和损失等info信息，实现：`helper.py`中的`report_training_losses_scales`

# 6.本地模型计算损失的过程

**实现**：`attack.py`中的`compute_blind_loss`

实现的思路如下：

1. 记录训练的任务是什么，是后门任务和正常任务
2. 获得后门训练批次，这里是加后门攻击的过程，实现：`synthesizer`中`make_backdoor_batch`
3. 利用多梯度下降算法来计算损失和梯度，实现：`losses.loss_functions`中的`compute_all_losses_and_grads`

## 1.多梯度下降算法MGDA

**前置知识：**

多梯度下降算法（Multi-Gradient Descent Algorithm，简称MGDA）是一种优化算法，旨在解决多目标优化问题，也就是在一个优化问题中存在多个目标函数需要最小化或最大化的情况。MGDA 的思路是通过协调多个目标函数的优化过程，以在不同目标之间取得平衡。

下面是 MGDA 算法的基本思路：

1. **多目标问题设定：** 首先，确定一个多目标优化问题，其中有多个待优化的目标函数。这里是正常任务和后门任务
2. **权衡策略：** 在 MGDA 中，需要定义一个**权衡策略**，该策略决定了如何将多个目标函数结合起来形成一个综合目标，以便进行单一的优化。这个权衡策略通常包括权重或权衡参数，用于调整不同目标函数的相对重要性。这里的权衡策略是什么？
3. **梯度计算：** 对于每个目标函数，计算其相对于优化变量的梯度。这意味着对每个目标函数进行单独的梯度计算。
4. **合并梯度：** 使用权衡策略将不同目标函数的梯度合并为一个综合梯度。这通常涉及将不同目标函数的梯度按照其权重进行加权求和。
5. **参数更新：** 使用合并的综合梯度来更新优化变量，通常使用标准的梯度下降或其他优化方法来实现。
6. **迭代优化：** 重复步骤 3 到步骤 5，直到满足停止条件（例如达到最大迭代次数或梯度阈值）。
7. **结果分析：** 最终的优化结果是一个在多个目标函数下达到权衡的解决方案。

MGDA 的主要挑战在于权衡策略的选择，因为不同的权衡策略可能会导致不同的最终解决方案。因此，需要根据具体问题的性质和需求来选择适当的权衡策略。

总的来说，MGDA 旨在处理多目标优化问题，通过协调不同目标函数的优化过程，寻找一个在多个目标之间取得平衡的解决方案。它在多领域的优化问题中都有应用，包括机器学习、控制系统、工程优化等领域。



以下是一个简单的例子，说明如何使用MGDA来解决一个多目标优化问题：

假设你是一家制造公司的经理，你需要决定生产两种不同的产品A和B。你有两个关键的目标：最大化产品A的利润和最大化产品B的销售量。然而，这两个目标之间存在权衡，因为提高产品A的利润可能会导致产品B的销售量下降，反之亦然。

问题设定如下：

- 目标1：最大化产品A的利润（表示为Profit_A）
- 目标2：最大化产品B的销售量（表示为Sales_B）

你可以使用MGDA来解决这个问题的优化过程：

1. 定义权衡策略：你需要定义一个权衡策略来决定在优化过程中分配多少资源（生产能力、广告预算等）给产品A和产品B。例如，你可以分配权重alpha给产品A的利润，权重(1-alpha)给产品B的销售量，其中alpha是一个介于0和1之间的参数，表示你对产品A和产品B的相对重要性。
2. 梯度计算：针对每个目标函数，计算其相对于资源分配（例如生产量、广告投入等）的梯度。这将涉及计算Profit_A和Sales_B关于资源分配的梯度。
3. 合并梯度：使用权衡策略中的权重alpha和(1-alpha)将两个目标函数的梯度加权合并为一个综合梯度。
4. 参数更新：使用合并的综合梯度来更新资源分配的变量。这可能涉及使用标准的梯度下降或其他优化算法来更新资源分配。
5. 迭代优化：重复步骤3和步骤4，直到达到停止条件，例如达到最大迭代次数或梯度变化很小。
6. 结果分析：最终的优化结果是一组资源分配，这些资源分配在产品A的利润和产品B的销售量之间取得了权衡。

通过调整权衡策略中的参数alpha，你可以探索不同的权衡点，以找到最适合公司目标的资源分配策略。这个例子说明了如何使用MGDA来处理多目标优化问题，并在不同目标之间进行权衡，以找到最佳解决方案。



**实现**：`losses.loss_functions`中的`compute_all_losses_and_grads`

**实现思路如下：**

1. 新建一个损失和梯度空字典
2. 对任务进行分类：
   1. 正常任务，实现：`losses.loss_functions`中的`compute_normal_loss`
   2. 后门任务，实现：`losses.loss_functions`中的`compute_backdoor_loss`



对于正常任务计算损失和梯度，思路如下：

1. 得到模型的输出
2. 利用交叉熵计算损失
3. 



# 7.得到本地上传更新的过程